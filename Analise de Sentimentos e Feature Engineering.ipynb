{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22cf1e4d",
   "metadata": {},
   "source": [
    "Este dataset trata-se de dados de companhias a√©reas dos EUA que cont√™m coment√°rios de passageiros com base no servi√ßo prestado pelas companhias a√©reas no Twitter.\n",
    "O link pode ser acessado em: https://www.kaggle.com/datasets/welkin10/airline-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aabc41",
   "metadata": {},
   "source": [
    "\n",
    "# Dicion√°rio de Dados - \n",
    "\n",
    "- **tweet_id**: Identificador √∫nico para cada tweet.\n",
    "- **airline_sentiment**: Sentimento do tweet em rela√ß√£o √† companhia a√©rea, podendo ser positivo, negativo ou neutro.\n",
    "- **airline_sentiment_confidence**: Grau de confian√ßa no sentimento atribu√≠do ao tweet, variando de 0 a 1, onde 1 indica alta confian√ßa.\n",
    "- **negativereason**: Raz√£o fornecida para o tweet ser classificado como negativo.\n",
    "- **negativereason_confidence**: Grau de confian√ßa na raz√£o fornecida para o tweet ser classificado como negativo, com valores variando de 0 a .\n",
    "- **airline**: Nome da companhia a√©rea a qual o tweet se refere.\n",
    "- **airline_sentiment_gold**: Sentimento atribu√≠do ao tweet por um avaliador humano de alta confian√ßa (padr√£o ouro).\n",
    "- **name**: Nome do usu√°rio que postou o tweet.\n",
    "- **negativereason_gold**: Raz√£o negativa atribu√≠da ao tweet por um avaliador humano de alta confian√ßa (padr√£o ouro).\n",
    "- **retweet_count**: N√∫mero de vezes que o tweet foi retweetado.\n",
    "- **text**: Texto do tweet.\n",
    "- **tweet_coord**: Coordenadas geogr√°ficas de onde o tweet foi postado.\n",
    "- **tweet_created**: Data e hora de cria√ß√£o do tweet.\n",
    "- **tweet_location**: Localiza√ß√£o mencionada no perfil do usu√°rio no momento da postagem do tweet.\n",
    "- **user_timezone**: Fuso hor√°rio do usu√°rio que postou o tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c95c1a",
   "metadata": {},
   "source": [
    "**1 - Selecionar um conjunto de dados de an√°lise de sentimentos bin√°rios dispon√≠vel em qualquer base de dados online, excluindo aqueles abordados em aula e no TP2. Implementar as t√©cnicas de TF-IDF, Bag-of-Words e Bag-of-nGrams para an√°lise dos dados. Ao concluir, apresentar uma an√°lise dos resultados obtidos. Caso o conjunto de dados apresente desbalanceamento, solicita-se a utiliza√ß√£o de m√©tricas adicionais al√©m da acur√°cia para avaliar o desempenho dos modelos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd9de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\giova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\giova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257562b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef9657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      14640 non-null  int64  \n",
      " 1   airline_sentiment             14640 non-null  object \n",
      " 2   airline_sentiment_confidence  14640 non-null  float64\n",
      " 3   negativereason                9178 non-null   object \n",
      " 4   negativereason_confidence     10522 non-null  float64\n",
      " 5   airline                       14640 non-null  object \n",
      " 6   airline_sentiment_gold        40 non-null     object \n",
      " 7   name                          14640 non-null  object \n",
      " 8   negativereason_gold           32 non-null     object \n",
      " 9   retweet_count                 14640 non-null  int64  \n",
      " 10  text                          14640 non-null  object \n",
      " 11  tweet_coord                   1019 non-null   object \n",
      " 12  tweet_created                 14640 non-null  object \n",
      " 13  tweet_location                9907 non-null   object \n",
      " 14  user_timezone                 9820 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7111edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecionado as colunas relevantes\n",
    "df = df[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf071f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c931b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9087\n",
      "neutral     3067\n",
      "positive    2298\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0e4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removando registros 'neutral' de 'airline_sentiment'\n",
    "df = df[df['airline_sentiment'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d06d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9087\n",
      "positive    2298\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48c340",
   "metadata": {},
   "source": [
    "Obs: Com base nos resultados acima, observamos que (ap√≥s a filtragem) possu√≠mos um conjunto desbalanceado onde 9087 men√ß√µes enquadram-se como **negativo** e 2298 como **positivo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b601fa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                 0\n",
       "airline_sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando valores NaN\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5328564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertendo a coluna airline sentment em bin√°rio\n",
    "df['airline_sentiment'] = df['airline_sentiment'].map({'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b716ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    9087\n",
      "1    2298\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32de89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iniciando o lematizador e stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fun√ß√£o para limpeza e lematiza√ß√£o do texto\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\@\\w+|\\#', '', text) # Removendo URLs, men√ß√µes e hashtags com regex\n",
    "    text = re.sub(r'\\W', ' ', text).lower() # Removendo caracteres n√£o alfanum√©ricos e convertendo para min√∫sculas\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]) # Lematizando e removendo stopwords\n",
    "    return text\n",
    "\n",
    "#Aplicando a fun√ß√£o\n",
    "df['clean_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f558fe57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "      <td>plus added commercial experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>0</td>\n",
       "      <td>seriously would pay 30 flight seat playing rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>1</td>\n",
       "      <td>yes nearly every time fly vx ear worm go away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14633</th>\n",
       "      <td>@AmericanAir my flight was Cancelled Flightled...</td>\n",
       "      <td>0</td>\n",
       "      <td>flight cancelled flightled leaving tomorrow mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>@AmericanAir right on cue with the delaysüëå</td>\n",
       "      <td>0</td>\n",
       "      <td>right cue delay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>1</td>\n",
       "      <td>thank got different flight chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>0</td>\n",
       "      <td>leaving 20 minute late flight warning communic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>money change flight answer phone suggestion ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11385 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  airline_sentiment  \\\n",
       "1      @VirginAmerica plus you've added commercials t...                  1   \n",
       "3      @VirginAmerica it's really aggressive to blast...                  0   \n",
       "4      @VirginAmerica and it's a really big bad thing...                  0   \n",
       "5      @VirginAmerica seriously would pay $30 a fligh...                  0   \n",
       "6      @VirginAmerica yes, nearly every time I fly VX...                  1   \n",
       "...                                                  ...                ...   \n",
       "14633  @AmericanAir my flight was Cancelled Flightled...                  0   \n",
       "14634         @AmericanAir right on cue with the delaysüëå                  0   \n",
       "14635  @AmericanAir thank you we got on a different f...                  1   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...                  0   \n",
       "14638  @AmericanAir you have my money, you change my ...                  0   \n",
       "\n",
       "                                              clean_text  \n",
       "1                 plus added commercial experience tacky  \n",
       "3      really aggressive blast obnoxious entertainmen...  \n",
       "4                                   really big bad thing  \n",
       "5      seriously would pay 30 flight seat playing rea...  \n",
       "6          yes nearly every time fly vx ear worm go away  \n",
       "...                                                  ...  \n",
       "14633  flight cancelled flightled leaving tomorrow mo...  \n",
       "14634                                    right cue delay  \n",
       "14635                 thank got different flight chicago  \n",
       "14636  leaving 20 minute late flight warning communic...  \n",
       "14638  money change flight answer phone suggestion ma...  \n",
       "\n",
       "[11385 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d3ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo os dados em treino e teste\n",
    "X = df['clean_text']\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a032326",
   "metadata": {},
   "source": [
    "**Obs:** Agora, iremos vetorizar o texto aplicando as 3 t√©cnicas propostas: Bag-of-Words, TF-IDF e Bag-of-nGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cadaabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo o treino e teste com Bag-of_words\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "457a6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo o treino e teste com TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b68bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo o treino e teste com Bag-of-nGrams\n",
    "vectorizer_ngrams = CountVectorizer(ngram_range=(1, 2))  \n",
    "X_train_ngrams = vectorizer_ngrams.fit_transform(X_train)\n",
    "X_test_ngrams = vectorizer_ngrams.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c471f34",
   "metadata": {},
   "source": [
    "Agora iremos aplicar um modelo de Regress√£o Log√≠stica em cada uma das t√©cnicas abordadas acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90fccfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalia√ß√£o do Modelo com Bag-of-Words\n",
      "Acur√°cia: 0.9073342116820378\n",
      "Precis√£o: 0.8075\n",
      "Recall: 0.7067833698030634\n",
      "F1-Score: 0.7537922987164527\n"
     ]
    }
   ],
   "source": [
    "#Modelo com Bag of words\n",
    "model_bow = LogisticRegression(max_iter = 1000)\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "#Realizando as predi√ß√µes\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "\n",
    "print(\"Avalia√ß√£o do Modelo com Bag of Words\")\n",
    "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"Precis√£o:\", precision_score(y_test, y_pred_bow))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_bow))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b03d8da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avalia√ß√£o do Modelo com TF-IDF\n",
      "Acur√°cia: 0.8959156785243741\n",
      "Precis√£o: 0.8819444444444444\n",
      "Recall: 0.5557986870897156\n",
      "F1-Score: 0.6818791946308725\n"
     ]
    }
   ],
   "source": [
    "#Modelo com TF-IDF\n",
    "model_tfidf = LogisticRegression(max_iter=1000)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#Realizando as prdedi√ß√µes\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nAvalia√ß√£o do Modelo com TF-IDF\")\n",
    "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Precis√£o:\", precision_score(y_test, y_pred_tfidf))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_tfidf))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f1ed8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avalia√ß√£o do Modelo com Bag of nGrams\n",
      "Acur√°cia: 0.9073342116820378\n",
      "Precis√£o: 0.8360655737704918\n",
      "Recall: 0.6695842450765864\n",
      "F1-Score: 0.7436208991494532\n"
     ]
    }
   ],
   "source": [
    "#Mmodelo com Bagof nGrams\n",
    "model_ngrams = LogisticRegression(max_iter=1000)\n",
    "model_ngrams.fit(X_train_ngrams, y_train)\n",
    "\n",
    "#Realuzando as predi√ß√µes\n",
    "y_pred_ngrams = model_ngrams.predict(X_test_ngrams)\n",
    "\n",
    "print(\"\\nAvalia√ß√£o do Modelo com Bag of nGrams\")\n",
    "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred_ngrams))\n",
    "print(\"Precis√£o:\", precision_score(y_test, y_pred_ngrams))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_ngrams))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_ngrams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7bd52",
   "metadata": {},
   "source": [
    "**Conclus√µes sobre o exerc√≠cio 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa0383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
